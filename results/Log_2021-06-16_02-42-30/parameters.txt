# -----------------------------------#
# Parameters of the training session #
# -----------------------------------#

# Input parameters
# ****************

dataset = ScannetSLAM
dataset_task = slam_segmentation
num_classes = 20
in_points_dim = 3
in_features_dim = 4
in_radius = 2.000000
input_threads = 10

# Model parameters
# ****************

architecture = simple resnetb resnetb_strided resnetb resnetb resnetb_strided resnetb_deformable resnetb_deformable resnetb_deformable_strided resnetb_deformable resnetb_deformable resnetb_deformable_strided resnetb_deformable resnetb_deformable nearest_upsample unary nearest_upsample unary nearest_upsample unary nearest_upsample unary
equivar_mode = 
invar_mode = 
num_layers = 5
first_features_dim = 64
use_batch_norm = 1
batch_norm_momentum = 0.020000

segmentation_ratio = 1.000000

# KPConv parameters
# *****************

first_subsampling_dl = 0.040000
num_kernel_points = 15
conv_radius = 2.500000
deform_radius = 6.000000
fixed_kernel_points = center
KP_extent = 1.000000
KP_influence = linear
aggregation_mode = sum
modulated = 0
n_frames = 1
max_in_points = 15344

max_val_points = 15344

val_radius = 2.000000

# Training parameters
# *******************

learning_rate = 0.010000
momentum = 0.980000
lr_decay_epochs = 1:0.984767 2:0.984767 3:0.984767 4:0.984767 5:0.984767 6:0.984767 7:0.984767 8:0.984767 9:0.984767 10:0.984767 11:0.984767 12:0.984767 13:0.984767 14:0.984767 15:0.984767 16:0.984767 17:0.984767 18:0.984767 19:0.984767 20:0.984767 21:0.984767 22:0.984767 23:0.984767 24:0.984767 25:0.984767 26:0.984767 27:0.984767 28:0.984767 29:0.984767 30:0.984767 31:0.984767 32:0.984767 33:0.984767 34:0.984767 35:0.984767 36:0.984767 37:0.984767 38:0.984767 39:0.984767 40:0.984767 41:0.984767 42:0.984767 43:0.984767 44:0.984767 45:0.984767 46:0.984767 47:0.984767 48:0.984767 49:0.984767 50:0.984767 51:0.984767 52:0.984767 53:0.984767 54:0.984767 55:0.984767 56:0.984767 57:0.984767 58:0.984767 59:0.984767 60:0.984767 61:0.984767 62:0.984767 63:0.984767 64:0.984767 65:0.984767 66:0.984767 67:0.984767 68:0.984767 69:0.984767 70:0.984767 71:0.984767 72:0.984767 73:0.984767 74:0.984767 75:0.984767 76:0.984767 77:0.984767 78:0.984767 79:0.984767 80:0.984767 81:0.984767 82:0.984767 83:0.984767 84:0.984767 85:0.984767 86:0.984767 87:0.984767 88:0.984767 89:0.984767 90:0.984767 91:0.984767 92:0.984767 93:0.984767 94:0.984767 95:0.984767 96:0.984767 97:0.984767 98:0.984767 99:0.984767 100:0.984767 101:0.984767 102:0.984767 103:0.984767 104:0.984767 105:0.984767 106:0.984767 107:0.984767 108:0.984767 109:0.984767 110:0.984767 111:0.984767 112:0.984767 113:0.984767 114:0.984767 115:0.984767 116:0.984767 117:0.984767 118:0.984767 119:0.984767 120:0.984767 121:0.984767 122:0.984767 123:0.984767 124:0.984767 125:0.984767 126:0.984767 127:0.984767 128:0.984767 129:0.984767 130:0.984767 131:0.984767 132:0.984767 133:0.984767 134:0.984767 135:0.984767 136:0.984767 137:0.984767 138:0.984767 139:0.984767 140:0.984767 141:0.984767 142:0.984767 143:0.984767 144:0.984767 145:0.984767 146:0.984767 147:0.984767 148:0.984767 149:0.984767 150:0.984767 151:0.984767 152:0.984767 153:0.984767 154:0.984767 155:0.984767 156:0.984767 157:0.984767 158:0.984767 159:0.984767 160:0.984767 161:0.984767 162:0.984767 163:0.984767 164:0.984767 165:0.984767 166:0.984767 167:0.984767 168:0.984767 169:0.984767 170:0.984767 171:0.984767 172:0.984767 173:0.984767 174:0.984767 175:0.984767 176:0.984767 177:0.984767 178:0.984767 179:0.984767 180:0.984767 181:0.984767 182:0.984767 183:0.984767 184:0.984767 185:0.984767 186:0.984767 187:0.984767 188:0.984767 189:0.984767 190:0.984767 191:0.984767 192:0.984767 193:0.984767 194:0.984767 195:0.984767 196:0.984767 197:0.984767 198:0.984767 199:0.984767 200:0.984767 201:0.984767 202:0.984767 203:0.984767 204:0.984767 205:0.984767 206:0.984767 207:0.984767 208:0.984767 209:0.984767 210:0.984767 211:0.984767 212:0.984767 213:0.984767 214:0.984767 215:0.984767 216:0.984767 217:0.984767 218:0.984767 219:0.984767 220:0.984767 221:0.984767 222:0.984767 223:0.984767 224:0.984767 225:0.984767 226:0.984767 227:0.984767 228:0.984767 229:0.984767 230:0.984767 231:0.984767 232:0.984767 233:0.984767 234:0.984767 235:0.984767 236:0.984767 237:0.984767 238:0.984767 239:0.984767 240:0.984767 241:0.984767 242:0.984767 243:0.984767 244:0.984767 245:0.984767 246:0.984767 247:0.984767 248:0.984767 249:0.984767 250:0.984767 251:0.984767 252:0.984767 253:0.984767 254:0.984767 255:0.984767 256:0.984767 257:0.984767 258:0.984767 259:0.984767 260:0.984767 261:0.984767 262:0.984767 263:0.984767 264:0.984767 265:0.984767 266:0.984767 267:0.984767 268:0.984767 269:0.984767 270:0.984767 271:0.984767 272:0.984767 273:0.984767 274:0.984767 275:0.984767 276:0.984767 277:0.984767 278:0.984767 279:0.984767 280:0.984767 281:0.984767 282:0.984767 283:0.984767 284:0.984767 285:0.984767 286:0.984767 287:0.984767 288:0.984767 289:0.984767 290:0.984767 291:0.984767 292:0.984767 293:0.984767 294:0.984767 295:0.984767 296:0.984767 297:0.984767 298:0.984767 299:0.984767 300:0.984767 301:0.984767 302:0.984767 303:0.984767 304:0.984767 305:0.984767 306:0.984767 307:0.984767 308:0.984767 309:0.984767 310:0.984767 311:0.984767 312:0.984767 313:0.984767 314:0.984767 315:0.984767 316:0.984767 317:0.984767 318:0.984767 319:0.984767 320:0.984767 321:0.984767 322:0.984767 323:0.984767 324:0.984767 325:0.984767 326:0.984767 327:0.984767 328:0.984767 329:0.984767 330:0.984767 331:0.984767 332:0.984767 333:0.984767 334:0.984767 335:0.984767 336:0.984767 337:0.984767 338:0.984767 339:0.984767 340:0.984767 341:0.984767 342:0.984767 343:0.984767 344:0.984767 345:0.984767 346:0.984767 347:0.984767 348:0.984767 349:0.984767 350:0.984767 351:0.984767 352:0.984767 353:0.984767 354:0.984767 355:0.984767 356:0.984767 357:0.984767 358:0.984767 359:0.984767 360:0.984767 361:0.984767 362:0.984767 363:0.984767 364:0.984767 365:0.984767 366:0.984767 367:0.984767 368:0.984767 369:0.984767 370:0.984767 371:0.984767 372:0.984767 373:0.984767 374:0.984767 375:0.984767 376:0.984767 377:0.984767 378:0.984767 379:0.984767 380:0.984767 381:0.984767 382:0.984767 383:0.984767 384:0.984767 385:0.984767 386:0.984767 387:0.984767 388:0.984767 389:0.984767 390:0.984767 391:0.984767 392:0.984767 393:0.984767 394:0.984767 395:0.984767 396:0.984767 397:0.984767 398:0.984767 399:0.984767 400:0.984767 401:0.984767 402:0.984767 403:0.984767 404:0.984767 405:0.984767 406:0.984767 407:0.984767 408:0.984767 409:0.984767 410:0.984767 411:0.984767 412:0.984767 413:0.984767 414:0.984767 415:0.984767 416:0.984767 417:0.984767 418:0.984767 419:0.984767 420:0.984767 421:0.984767 422:0.984767 423:0.984767 424:0.984767 425:0.984767 426:0.984767 427:0.984767 428:0.984767 429:0.984767 430:0.984767 431:0.984767 432:0.984767 433:0.984767 434:0.984767 435:0.984767 436:0.984767 437:0.984767 438:0.984767 439:0.984767 440:0.984767 441:0.984767 442:0.984767 443:0.984767 444:0.984767 445:0.984767 446:0.984767 447:0.984767 448:0.984767 449:0.984767 450:0.984767 451:0.984767 452:0.984767 453:0.984767 454:0.984767 455:0.984767 456:0.984767 457:0.984767 458:0.984767 459:0.984767 460:0.984767 461:0.984767 462:0.984767 463:0.984767 464:0.984767 465:0.984767 466:0.984767 467:0.984767 468:0.984767 469:0.984767 470:0.984767 471:0.984767 472:0.984767 473:0.984767 474:0.984767 475:0.984767 476:0.984767 477:0.984767 478:0.984767 479:0.984767 480:0.984767 481:0.984767 482:0.984767 483:0.984767 484:0.984767 485:0.984767 486:0.984767 487:0.984767 488:0.984767 489:0.984767 490:0.984767 491:0.984767 492:0.984767 493:0.984767 494:0.984767 495:0.984767 496:0.984767 497:0.984767 498:0.984767 499:0.984767
grad_clip_norm = 100.000000

augment_symmetries = 1 0 0
augment_rotation = vertical
augment_noise = 0.001000
augment_occlusion = none
augment_occlusion_ratio = 0.200000
augment_occlusion_num = 1
augment_scale_anisotropic = 1
augment_scale_min = 0.800000
augment_scale_max = 1.200000
augment_color = 1.000000

weight_decay = 0.001000
segloss_balance = none
class_w =
deform_fitting_mode = point2point
deform_fitting_power = 1.000000
deform_lr_factor = 0.100000
repulse_extent = 1.200000
batch_num = 8
val_batch_num = 8
max_epoch = 500
epoch_steps = 600
validation_size = 100
checkpoint_gap = 50
